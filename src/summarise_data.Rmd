---
title: "Summary of psychological ownership survey answers"
author: "Abigail Edwards and Mikko Arvas"
date: "`r Sys.time()`"
output:
  html_document:
    toc: yes
    theme: united
  pdf_document:
    toc: yes
---


TODO:

- is the null model correct ?

- should some other parameters be set to SEM fitting?


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidyverse)
library(gtsummary)
library(likert)
library(readxl)
library(here)
library(corrplot)
library(ggstats)
library(lavaan)
library(lavaanPlot)
#library(blavaan) could get to work with this model
#library(manymome) # could have interesting other statistics to look at

# https://github.com/cjvanlissa/worcs
# https://cjvanlissa.github.io/worcs/articles/setup.html
# worcs::check_worcs_installation() 

#install.packages("tinytex", dependencies = TRUE)
#library("tinytex")
#install_tinytex()


# https://cjvanlissa.github.io/worcs/reference/add_preregistration.html which template ? 
# https://github.com/crsh/prereg
```


# Data

## Import

```{r}
# Read real data in 
# file <- here("data/Tulokset - xxxxx.xlsx")
# data <- read_xlsx(file,n_max = XX)
# summary(data)
```

Items
<table>
<tr>
<td></td>
<td> Psychological ownership</td>
</tr>
<tr>
<td>po01</td>
<td>
1. I feel a very high degree of personal ownership of donating blood.
</td>
</tr>
<tr>
<td>po02</td>
<td>
2. I feel like donating blood belongs to me.
</td> </tr>
<tr>
<td>po03</td> <td>
3. I feel like I own donating blood.
</td> </tr>
<tr> <td></td>

<td>
Self-identity
</td> </tr>
<tr>
<td>si01</td>
<td>
1. Donating blood is important to me.
</td> <tr>
<td>si02</td> <td>
2. I am like the kind of person who donates blood.
</td> </tr>
<tr> <td>si03</td> 
<td>
3. Donating blood is an important part of who I am.
</td> </tr>
<tr> <td></td>

<td>
Intention
</td></tr>
<tr><td>it01</td>
<td>
1. I would like to donate blood in the future.
</td>   </tr>
<tr><td>it02</td>
<td>
2. I intend to donate blood in the future.
</td> </tr>
<tr><td>it03</td>

<td>
3. I intend to make more than just a one-off blood donation.
</td></tr>
<tr><td></td>
<td>
Intention to donate for emergency
</td></tr>
<tr><td>ie01</td>
<td>
1. I would be willing to donate blood if I received an urgent callout from the Finnish Red Cross Blood Service.
</td></tr>
<tr><td></td> 
<td>
Antecedents of psychological ownership
</td></tr>
<tr><td></td> 

<td>
Control
</td></tr>
<tr><td>ac01</td>
<td>
1. I have influence over the things that affect me while donating blood.
</td></tr>
<tr><td>ac02</td>
<td>
2. I control the location and scheduling for donating blood.
</td></tr>
<tr><td>ac03</td>
<td>
3. I influence decisions related to my blood donation.
</td></tr>
<tr><td>ac04</td>
<td>
4. In general, I have control over donating blood.
</td></tr>
<tr><td></td>
<td>
Intimate knowledge
</td></tr>
<tr><td>ai01</td>

<td>
1. I am intimately familiar with what is going on with regard to donating blood.
</td></tr>
<tr><td>ai02</td>
<td>
2. I have a depth of knowledge as it relates to donating blood.
</td></tr>
<tr><td>ai03</td>
<td>
3. I have a comprehensive understanding of donating blood.
</td></tr>
<tr><td>ai04</td>
<td>
4. I have a broad understanding of donating blood.
</td></tr>
<tr><td></td>

<td>
Self-Investment
</td></tr>
<tr><td>iv01</td>
<td>
1.	I have invested a major part of myself into donating blood.
</td></tr>
<tr><td>iv02</td>
<td>
2. I have invested a significant amount of my time into donating blood.
</td></tr>
<tr><td>iv03</td>
<td>
3. Overall, I invested a lot into donating blood.
</td></tr>
<tr><td></td>


<td>
Blood donation history
</td></tr>
<tr><td>bd01</td>
<td>
1. Do you believe that you are currently eligible to donate blood [yes, no, unsure]
</td></tr>
<tr><td>bd02</td>
<td>
2. How many times in total have you donated blood at the Finnish Red Cross Blood Service Finland? [select number]
</td></tr>
<tr><td>bd03</td>
<td>
3. How many times in the last two years have you donated at the Finnish Red Cross Blood Service? [select number]
</td></tr>
<tr><td>bd04</td>
<td>
4. When did you last attend a Finnish Red Cross blood donation centre with the intention of donating blood? [select date]
</td></tr>
<tr><td></td>

<td>
Demographics
</td></tr>
<tr><td>de01</td>
<td>1. What is your gender? [man, woman, non-binary, prefer to not disclose, prefer to self-describe (open response)]
</td></tr>
<tr><td>de02</td>
<td>
2. How old are you today? [select number]
</td></tr>
</table>


```{r}
#Make question abbreviations
qabbr <- c(
    str_c("po0",1:3),
    str_c("si0",1:3),
    str_c("it0",1:3),
    str_c("ie0",1),
    str_c("ac0",1:4),
    str_c("ai0",1:4),
    str_c("iv0",1:3),
    str_c("bd0",1:4),
    str_c("de0",1:2)
  )

```


```{r}
#With the simulated data sem fitting with data containing factors is unstable, hence here is data and models from that fit.
datafile <- "workingexample20250303.data.rdata"
workingexample <- TRUE # TRUE for working with presimulated data
fitmodels <- TRUE # FALSE for working with prefitted models
if (workingexample) {
  load(file=here(str_c("results/",datafile)))
}
if (!fitmodels) {
  load(file=here(str_replace(str_c("results/",datafile),"\\.data\\.",".models.")))
  fit.po02 <- models$fit.po02
  fit.po02null <- models$fit.po02null
  fit.po03 <- models$fit.po03
  fit.po03null <- models$fit.po03null
}


```


## Simulate

All items are scored 1 (strongly disagree) to 7 (strongly agree).

```{r}
if (!workingexample) { # if you have data you set this to false
  n <- 1000
  #source("../../responsesR/R/responses.R")
  #create correlation matrix
  #po 3, si 3,  it 3, ie 1, ac 4,ai 4, in 3 
  R <- matrix(
  #     po          si        it        ie         ac        ai              in
  c( 1 ,0.5,0.5,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.5, 1 ,0.5,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.5,0.5, 1 ,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2, 1 ,0.5,0.5,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2,0.5, 1 ,0.5,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2,0.5,0.5, 1 ,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2, 1 ,0.5,0.5,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.5, 1 ,0.5,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.5,0.5, 1 ,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2, 1 ,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2, 1 ,0.5,0.5,0.5,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.5, 1 ,0.5,0.5,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.5,0.5, 1 ,0.5,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.5,0.5,0.5, 1 ,0.2,0.2,0.2,0.2,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2, 1 ,0.5,0.5,0.5,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.5, 1 ,0.5,0.5,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.5,0.5, 1 ,0.5,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.5,0.5,0.5, 1 ,0.2,0.2,0.2,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2, 1 ,0.5,0.5,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.5, 1 ,0.5,
    0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.5,0.5, 1 
    # bd and de are different
    )
  , nrow=21)
    # if you have problems name the columns and rows here
  rownames(R) <- qabbr[1:21]
  colnames(R) <- qabbr[1:21]
  corrplot(R)
  # Only correlations between latents 
}
```




```{r}
#simulate data with responsesR::get_responses
if (!workingexample) {
library(responsesR)
  
  data_likert_7<- get_responses(n = n,
                K = 7,
                mu = rep(1,nrow(R)),
                gamma1 = 0.1,
                sd = 1,
                R=R
  )
  
  de02 <- round(rnorm(n=n,mean=40,sd =10)) # age
  #de01<- genLikert(size = n,items = 1, levels=4, location=c(-1)) #gender
  de01 <- get_responses(n = n,
                K = 4,
                mu = -1,
                gamma1 = 0.1,
                sd = 0.3
  )
  de01 <-     
      case_when(
      de01 == 1 ~ "women",
      de01 == 2 ~ "man",
      de01 == 3 ~ "Non-binary/other",
      de01 == 4 ~ "Don't wish to respond")
  #bd01 <-  genLikert(size = n,items = 1, levels=3, location=c(-1))
  bd01 <- get_responses(n = n,
                        K = 3,
                        mu = -1,
                        gamma1 = 0.3,
                        sd = 0.7
  )
  bd01 <- case_when(
    bd01 == 1 ~ "yes",
    bd01 == 2 ~ "no",
    bd01 == 3 ~ "unsure",
  )
  data <- bind_cols(
    data_likert_7,
    bd01,
    bd02 = NA,
    bd03 = NA,
    bd04 = NA,
    de01,
    de02
  )
  #colnames(data) <- qabbr[1:ncol(data)]
  colnames(data) <- qabbr
  data <- data %>% 
    mutate(
      # to have some relationship between latent variables make donation counts and po correlate
       bd02 = 
      #take this from po 
         round(as.numeric(po01)  +  
                 as.numeric(po02)   + 
                 as.numeric(data$po03) +
      # and sex
                 0.3*as.numeric(as.factor(de01))   + 
      #with some noise
                 rnorm(nrow(data),0,4) ),
      #make sure it never goes below 1
      bd02 = ifelse(bd02 < 2 ,1,bd02),
      # and count donation in last two years from it with some noise
      bd03 = round(ceiling(bd02 / 5) + rnorm(nrow(data),0,1)),
      bd03 = ifelse(bd03 < 1 ,1,bd03),
      bd04 = sample(seq(as.Date('2022/01/01'), as.Date('2024/01/01'), by="day"), n, replace = TRUE),
      de02 = ifelse(de02 < 18 ,18,de02),
      de02 = ifelse(de02 > 70 ,70,de02)
    )
  
  
  
  
  save(data, file=here(str_c("results/",datafile)))
  summary(data)
} 

```



## Process

```{r}


f1 <- function (x){
  #as numbers but labelled with words
    x <- factor(
    x,
    #levels = c("Strongly disagree","Disagree","Neutral","Agree","Strongly agree"),
    levels = c(1:7),
    ordered = TRUE
  )
  x
}

data <- data %>% 
  mutate_at(
    vars(
      qabbr[1:21]),
    ~ f1(.) 
    ) %>% 
  mutate(bd04 = as.Date(bd04),
         bd01 = factor(bd01, levels = c("no","unsure","yes"),ordered = TRUE),
         de01 = factor(de01)
         )  




summary(data)
```


# Plot



## Demographics Table 1

```{r}

table1 <-
  tbl_summary(
    data %>% 
      # mutate(
      #   bd01 = as.character(bd01) # this did fix this need to think of something
      # ) %>% 
      rename(
      Gender =  de01,
      Age = de02,
      DonationEligibility = bd01,
      DonationCount = bd02,
      DonationCountLast2years = bd03,
    ),
    include = c(
      Age,
      DonationEligibility,
      DonationCount,
      DonationCountLast2years
      ),
    by =  Gender
    
  ) %>%
  add_n()  # add column with total number of non-missing observations
table1
```



## Age by Gender

```{r}

p <- ggplot(data)
p <- p +  geom_histogram(aes(x=de02),binwidth = 5)
p <- p + facet_wrap(. ~ de01)
p <- p + xlab("Age in years")
p

```



## Psychological ownership


```{r}
gglikert(
  data %>% 
    select(which(str_detect(colnames(data) ,"^po"))) 
)
```




## Self-identity


```{r}
gglikert(
  data %>% 
    select(which(str_detect(colnames(data) ,"^si"))) 
)
```


## Intention

```{r}
gglikert(
  data %>% 
    select(which(str_detect(colnames(data) ,"^it"))) 
)
```


## Intention to donate for emergency

```{r}
gglikert(
  data %>% 
    select(which(str_detect(colnames(data) ,"^ie"))) 
)
```



## Antecedents of psychological ownership - Control

```{r}
gglikert(
  data %>% 
    select(which(str_detect(colnames(data) ,"^ac"))) 
)
```

## Antecedents of psychological ownership - Intimate knowledge


```{r}
gglikert(
  data %>% 
    select(which(str_detect(colnames(data) ,"^ai"))) 
)
```


## Antecedents of psychological ownership - Self-Investment


```{r}
gglikert(
  data %>% 
    select(which(str_detect(colnames(data) ,"^iv"))) 
)
```



# Model


```{r}

library(lavaan)
#library(blavaan) # tidySEM requires blavaan
#library(tidySEM) # https://github.com/yrosseel/lavaan/issues/359 lavaan does not work if tidySEM is loaded.

```


## Confirmatory factor analysis

```{r}
models <- list()

isordered <- data %>%  summarise(across(everything(), ~ is.ordered(.x))) %>%  as.logical() %>% which()
isfactor <- data %>%  summarise(across(everything(), ~ is.factor(.x))) %>%  as.logical() %>% which()

#lavaan does not understand empty factor levels
data.fit <- data %>%
  #lavaan cannot deal with unordered factor with more then 2 levels
  filter(de01 == 'man' | de01 == "women") %>% 
  mutate(
    across(isfactor, ~ fct_drop(.x)
    ) %>% 
      mutate(
        bd04 = as.numeric(bd04), # lavaan cannot handle dates
        bd04 = bd04 / 10^-(2 - str_length(as.numeric(max(bd04)))) # https://groups.google.com/g/lavaan/c/r7w-4HHg5R0 
        #"Your underweight variance seems to be much larger than other variances.  Try dividing it by 10 or 100 to make the SDs more similar across modeled variables." 
        # But this does not seem to necessarily help.
      )
  )

# To make bootstrapping run there cannot be any unordered factors
data.fit2 <- data.fit %>% 
  mutate(
    de01 = as.ordered(de01)
  ) %>% # and neither any factors at all
mutate(
  across(everything(),as.numeric)
)  # but this also good for plotting correlations


summary(data.fit)
```


```{r}
corrplot(cor(data.fit2))
```



```{r}
po01.model <- ' po =~ po01 + po02 + po03
                si =~ si01 + si02 + si03
                it =~ it01 + it02 + it03
                ac =~ ac01 + ac02 + ac03 + ac04
                ai =~ ai01 + ai02 + ai03 + ai04 
                iv =~ iv01 + iv02 + iv03
                #bd =~ bd01 + bd02 + bd03 + bd04
                bd =~ bd02 + bd01 + bd03 + bd04 #bd02 is sure correlate positively

'
                
fit.po01.cfa <- cfa(po01.model,  
                    estimator = "WLSMV", #
                    
                    data = data.fit)
fit.po01.cfa
# Minimum function test statistic is chi^2
```


```{r}
lavaan::summary(fit.po01.cfa,standardized = TRUE) # tidySEM is loaded this fails!
#lavaan::summary(fit.po01.cfa) # tidySEM is loaded this fails!

#The estimate is the non-standardised as can be seen from the fact that
#the first item is always 1. Then the standardised are in the "Std" columns.
#"Std.all" is "factor loading" and crude rule if thumb is that abs(Std.all) should be above 0.3
#With simulated data bd04 is just random so it should be smaller than others.
```

```{r}
#lavaan::parameterestimates(fit.po02)
# Check items which correlate poorly with their latent factor
cfastd <- lavaan::standardizedsolution(fit.po01.cfa)
cfastd %>% 
  filter(abs(est.std) < 0.3) %>% 
  select(-z,-se) %>% 
  arrange(abs(est.std)) %>% 
  # Just the loadings
  filter(op != '~1')

```




```{r}
lavaan::fitmeasures(fit.po01.cfa,c("npar","chisq","df","cfi","rmsea","srmr"))
```

https://stats.stackexchange.com/questions/541707/cfa-in-lavaan-wont-converge 
"For the comparative fit indices CFI and TLI, you need at least >.900. For the absolute fit indices RMSEA and SRMR, you should have <.080."
So ones from simulated data look good enough.

```{r , fig.height=20}
lavaanPlot(
  fit.po01.cfa,
   edge_options = list(color = "grey"),
  coefs = TRUE,
  graph_options = list(rankdir = "LR")
           )
```


```{r}
#Make a model with out prespesified correlation structure
fit.po01.cfa.o <- cfa(po01.model,data.fit,orthogonal=TRUE)
round(
  sapply(list(cfa=fit.po01.cfa,cfa.o=fit.po01.cfa.o),
         function(x) fitmeasures(x,c("npar","chisq","df","cfi","rmsea","srmr"))
         )
,3)

#cfa should be better than cfa.o in all measures. cfi and chisq should be bigger, rmsea and srmr smaller in model cfa
```

```{r}
#anova(fit.po01.cfa,fit.po01.cfa.o)
#Error: lavaan->lavTestLRT(): 
#some models (but not all) have scaled test statistics
```



## SEM

### Null model for SEM

```{r}
#https://stats.stackexchange.com/questions/340857/serial-mediation-in-r-how-to-setup-the-model

# N.B. you need to check that first item is positively correlated with your latent factor
# Otherwise you get opposite direction 

po02null.model <- ' 
#measurement model
#psycown =~ po01 + po02 + po03 #NO PSYCOWN
selfid =~ si01 + si02 + si03
intention =~ it01 + it02 + it03
##psycown_cont =~ ac01 + ac02 + ac03 + ac04 # these were not used in Edwards23
##psycown_know =~ ai01 + ai02 + ai03 + ai04 # these were not used in Edwards23  
blooddonor =~ bd02 + bd01 + bd03 +bd04 # do we want to include the DATE OF LAST DONATION?

#mediation
#psycown ~ a1 *  blooddonor #NO PSYCOWN 

#selfid ~ a2 * blooddonor + d21 * psycown
selfid ~ a2 * blooddonor # #NO PSYCOWN

#regression
intention ~  de01 + de02 + ie01 +  cp * blooddonor  + b2 * selfid #NO PSYCOWN
#intention ~  de01 + de02 + ie01 +  cp * blooddonor + b1 * psycown + b2 * selfid

#ind_eff := a1* d21 * b2 #NO PSYCOWN
'

cat(po02null.model)

```


```{r}


if (fitmodels) {
#fit
#https://web.pdx.edu/~newsomj/semclass/ho_categorical.pdf
# "There seems to be growing consensus among researchers that the best approach to analysis of
# binary and ordinal variables (with few categories) is what is referred most to most commonly as diagonally
# weighted least squares (DWLS) approach (Muthén, du Toit, & Spisic, 1997). This estimation method is
# called weighted least squares mean and variance adjusted (WLSMV) in Mplus and the R package lavaan
# (it is invoked by estimator = WLSMV). "

#https://www.tandfonline.com/doi/full/10.1080/10705511.2021.1877548                
fit.po02null <- sem(po02null.model,  
                #default maximum likelihood (ML, MLR,..) methods don't work with categorials
                 #estimator = "DWLS", # diagonally weighted least squares, often recomended
                    estimator = "WLSMV", # robust diagonally weighted least squares stable and should work with ordered factors
                #We select WLSMV as the estimator. Often it does not fit with the simulated data.
                #It might work with real data and then we compare ML estimates from bootstrapped.
                                 data = data.fit
                
                #SHOULD SOME OTHER PARAMETERS BE SET?
                
                #The sem function is a wrapper for the more general lavaan function, but setting the following default options:
                # int.ov.free = TRUE, If FALSE, the intercepts of the observed variables are fixed to zero.
                # int.lv.free = FALSE, If FALSE, the intercepts of the latent variables are fixed to zero.
                # auto.fix.first = TRUE (unless std.lv = TRUE),  If TRUE, the factor loading of the first indicator is set to 1.0 for every latent variable.
                # auto.fix.single = TRUE, If TRUE, the residual variance (if included) of an observed indicator is set to zero if it is the only indicator of a latent variable.
                # auto.var = TRUE, If TRUE, the (residual) variances of both observed and latent variables are set free.
                # auto.cov.lv.x = TRUE, If TRUE, the covariances of exogenous latent variables are included in the model and set free.
                # auto.efa = TRUE, 
                # auto.th = TRUE, 
                # auto.delta = TRUE,
                # auto.cov.y = TRUE If TRUE, the covariances of dependent variables (both observed and latent) are included in the model and set free.
                  )


#This is quite unstable. Sometimes it says:
#"lavaan 0.6-19 did NOT end normally after nn iterations"
#and sometimes
#"lavaan 0.6-19 ended normally after nn iterations"
#You need to have enough data n~1000?
#
models[["fit.po02null"]] <- fit.po02null
}

lavaan::summary(fit.po02null,standardized = TRUE)

```

```{r}
varTable(fit.po02null)
```

IS ie01 a problem? should it be numeric?
IS bd04 a problem? should we reduce its variation by scaling it down?



```{r}
#inspect(fit,what="")
lavaan::inspect(fit.po02null,"r2")
```



```{r}
lavaan::standardizedsolution(fit.po02null) %>% select(-label)
# Use standardizedSolution to obtain SEs and test statistics for standardized estimates. 

```

```{r}
lavaan::standardizedsolution(fit.po02null) %>% select(-label,se,z) %>%  filter(op == '~')
```



```{r}

fit.po02null.est  <-standardizedSolution(fit.po02null)
fit.po02null.est %>% 
  filter(abs(est.std) < 0.3) %>% 
  select(-z,-se) %>% 
  arrange(abs(est.std)) %>% 
  # Just the loadings
  filter(op != '~1')

```



```{r}

labelsforplotting <- c(
  #"psycown" = "Psychological ownership",
  "selfid"  = "Self-identity",
  "intention" = "Intention",
  "blooddonor " = "Blood donation history"
)

lavaanPlot(
  model=fit.po02null,  
  coefs = TRUE,
  node_options = list(shape = "box", fontname = "Helvetica"), 
  edge_options = list(color = "grey"), 
  labels = labelsforplotting,

  #In simulated data there should be correlations inside
  # bloodonor, psycown, selfid
  # and between
  # bloodonor, psycown
  # and nothing else
  sig = 0.001
  # if sig is < 0.001 one gets significant correlations
  # also from simulated data elsewhere suggesting that 0.05 is not enough
  
  )
```

```{r}
# Same with standardised coefficients
lavaanPlot(
  model = fit.po02null, 
   labels = labelsforplotting,
  node_options = list(shape = "box", fontname = "Helvetica"), 
  edge_options = list(color = "grey"), 
  coefs = TRUE, 
  stand = TRUE,
  sig=0.001
  )
```



### SEM

```{r}
#https://stats.stackexchange.com/questions/340857/serial-mediation-in-r-how-to-setup-the-model

# N.B. you need to check that first item is positively correlated with your latent factor
# Otherwise you get opposite direction 

po02.model <- ' 
#measurement model
psycown =~ po01 + po02 + po03
selfid =~ si01 + si02 + si03
intention =~ it01 + it02 + it03
#psycown_cont =~ ac01 + ac02 + ac03 + ac04 # these were not used in Edwards23
#psycown_know =~ ai01 + ai02 + ai03 + ai04 # these were not used in Edwards23
blooddonor =~ bd02 + bd01 + bd03 +bd04 # do we want to include the DATE OF LAST DONATION?

#mediation
psycown ~ a1 *  blooddonor
selfid ~ a2 * blooddonor + d21 * psycown

#regression
intention ~  de01 + de02 + ie01 +  cp * blooddonor + b1 * psycown + b2 * selfid
ind_eff := a1* d21 * b2
'

cat(po02.model)

```


```{r}


if (fitmodels) {
#fit
#https://web.pdx.edu/~newsomj/semclass/ho_categorical.pdf
# "There seems to be growing consensus among researchers that the best approach to analysis of
# binary and ordinal variables (with few categories) is what is referred most to most commonly as diagonally
# weighted least squares (DWLS) approach (Muthén, du Toit, & Spisic, 1997). This estimation method is
# called weighted least squares mean and variance adjusted (WLSMV) in Mplus and the R package lavaan
# (it is invoked by estimator = WLSMV). "

#https://www.tandfonline.com/doi/full/10.1080/10705511.2021.1877548                
fit.po02 <- sem(po02.model,  
                #default maximum likelihood (ML, MLR,..) methods don't work with categorials
                 #estimator = "DWLS", # diagonally weighted least squares, often recomended
                    estimator = "WLSMV", # robust diagonally weighted least squares stable and should work with ordered factors
                #We select WLSMV as the estimator. Often it does not fit with the simulated data.
                #It might work with real data and then we compare ML estimates from bootstrapped.
                                 data = data.fit
                
                #SHOULD SOME OTHER PARAMETERS BE SET?
                
                #The sem function is a wrapper for the more general lavaan function, but setting the following default options:
                # int.ov.free = TRUE, If FALSE, the intercepts of the observed variables are fixed to zero.
                # int.lv.free = FALSE, If FALSE, the intercepts of the latent variables are fixed to zero.
                # auto.fix.first = TRUE (unless std.lv = TRUE),  If TRUE, the factor loading of the first indicator is set to 1.0 for every latent variable.
                # auto.fix.single = TRUE, If TRUE, the residual variance (if included) of an observed indicator is set to zero if it is the only indicator of a latent variable.
                # auto.var = TRUE, If TRUE, the (residual) variances of both observed and latent variables are set free.
                # auto.cov.lv.x = TRUE, If TRUE, the covariances of exogenous latent variables are included in the model and set free.
                # auto.efa = TRUE, 
                # auto.th = TRUE, 
                # auto.delta = TRUE,
                # auto.cov.y = TRUE If TRUE, the covariances of dependent variables (both observed and latent) are included in the model and set free.
                  )


#This is quite unstable. Sometimes it says:
#"lavaan 0.6-19 did NOT end normally after nn iterations"
#and sometimes
#"lavaan 0.6-19 ended normally after nn iterations"
models[["fit.po02"]] <- fit.po02
}

lavaan::summary(fit.po02,standardized = TRUE)

```

```{r}
varTable(fit.po02)
```

IS ie01 a problem? should it be numeric?
IS bd04 a problem? should we reduce its variation by scaling it down?



```{r}
#inspect(fit,what="")
lavaan::inspect(fit.po02,"r2")
```



```{r}
lavaan::standardizedsolution(fit.po02) %>% select(-label)
# Use standardizedSolution to obtain SEs and test statistics for standardized estimates. 

```

```{r}
lavaan::standardizedsolution(fit.po02) %>% select(-label,se,z) %>%  filter(op == '~')
```



```{r}

fit.po02.est  <-standardizedSolution(fit.po02)
fit.po02.est %>% 
  filter(abs(est.std) < 0.3) %>% 
  select(-z,-se) %>% 
  arrange(abs(est.std)) %>% 
  # Just the loadings
  filter(op != '~1')

```



```{r}

labelsforplotting <- c(
  "psycown" = "Psychological ownership",
  "selfid"  = "Self-identity",
  "intention" = "Intention",
  "blooddonor " = "Blood donation history"
)

lavaanPlot(
  model=fit.po02,  
  coefs = TRUE,
  node_options = list(shape = "box", fontname = "Helvetica"), 
  edge_options = list(color = "grey"), 
  labels = labelsforplotting,

  #In simulated data there should be correlations inside
  # bloodonor, psycown, selfid
  # and between
  # bloodonor, psycown
  # and nothing else
  sig = 0.001
  # if sig is < 0.001 one gets significant correlations
  # also from simulated data elsewhere suggesting that 0.05 is not enough
  
  )
```

```{r}
# Same with standardised coefficients
lavaanPlot(
  model = fit.po02, 
   labels = labelsforplotting,
  node_options = list(shape = "box", fontname = "Helvetica"), 
  edge_options = list(color = "grey"), 
  coefs = TRUE, 
  stand = TRUE,
  sig=0.001
  )
```



### Null SEM with bootstrapping

```{r}
nBoots <- 1000            

if (fitmodels) {
#fit

# https://groups.google.com/g/lavaan/c/A6KCjXAZl-Q good example code

fit.po03null <- sem(po02null.model,  
                    estimator = "ML", # bootstrapping does not work with estimators designed for factors
                    data = data.fit2, # data that is completely numeric
                bootstrap = nBoots,
                se = "boot"
                )
models[['fit.po03null']] <- fit.po03null
}

lavaan::summary(fit.po03null,standardized = TRUE)

```



```{r}
lavaanPlot(
  model = fit.po03null, 
   labels = labelsforplotting,
  node_options = list(shape = "box", fontname = "Helvetica"), 
  edge_options = list(color = "grey"), 
  coefs = TRUE, 
  stand = TRUE,
  sig=0.001
  )


```

```{r}
fit.po03null.best <- parameterEstimates(fit.po03null)
fit.po03null.best %>% select(-se,-z) %>% filter(op == "~")
```



### SEM with bootstrapping

```{r}

if (fitmodels) {
#fit

# https://groups.google.com/g/lavaan/c/A6KCjXAZl-Q good example code

fit.po03 <- sem(po02.model,  
                    estimator = "ML", # bootstrapping does not work with estimators designed for factors
                    data = data.fit2, # data that is completely numeric
                bootstrap = nBoots,
                se = "boot"
                )
models[['fit.po03']] <- fit.po03
}

lavaan::summary(fit.po03,standardized = TRUE)

```



```{r}
lavaanPlot(
  model = fit.po03, 
   labels = labelsforplotting,
  node_options = list(shape = "box", fontname = "Helvetica"), 
  edge_options = list(color = "grey"), 
  coefs = TRUE, 
  stand = TRUE,
  sig=0.001
  )


```

```{r}
fit.po03.best <- parameterEstimates(fit.po03)
fit.po03.best %>% select(-se,-z) %>% filter(op == "~")
```


## Compare models


```{r}
data.forest  <- fit.po03.best %>% 
  mutate(estmet = "Hypothesis bootstrapped") %>%  
  filter(op == "~") %>% 
  bind_rows(
    fit.po02.est %>% mutate(estmet = "Hypothesis") %>% 
      rename(est=est.std) %>% 
      filter(op == "~") 
  ) %>% 
  mutate(
    term = str_c(label,":",lhs,op,rhs)
    ) %>% 
  arrange(estmet, term)

data.forest  <- fit.po03null.best %>% 
  mutate(estmet = "Null bootstrapped") %>%  
  filter(op == "~") %>% 
  bind_rows(
    fit.po02null.est %>% mutate(estmet = "Null") %>% 
      rename(est=est.std) %>% 
      filter(op == "~") 
  ) %>% 
  mutate(
    term = str_c(label,":",lhs,op,rhs)
    ) %>% 
  arrange(estmet, term) %>% 
  bind_rows(data.forest)



pos <- position_nudge(y=as.numeric(as.factor(data.forest$estmet))/5 -0.5) 

data.forest %>% 
  mutate(
    hollow_color = ifelse(ci.lower < 0 & ci.upper > 0, NA, "black")
  ) %>% 
  ggplot(aes(est,xmin=ci.lower,xmax=ci.upper,y=term,fill=I(hollow_color),color=estmet))+
  geom_vline(xintercept=0,color="grey") +
  geom_linerange(position=pos)+
  geom_point(shape=21,siz=3,position=pos)+
  scale_fill_discrete(na.value=NA) + 
  labs(x="Standardised effect size",y="Relationship") +
  labs(color="Estimation\n method")


```

```{r}

round(
  sapply(list(Null=fit.po02null,NullBoot=fit.po03null,Hypo=fit.po02,HypoBoot=fit.po03 ),
         function(x) fitmeasures(x,c("chisq","df","cfi","rmsea","srmr"))
         )
,3)

```

According to https://easystats.github.io/effectsize/reference/interpret_gfi.html


"For structural equation models (SEM), Kline (2015) suggests that at a minimum the following indices should be reported: The model chi-square, the RMSEA, the CFI and the SRMR."

chisq, Chi-squared, larger is better

cfi Comparative Fit Index, "It should be > .96" , higher is better

rmsea Root Mean Square Error of Approximation, "should be < .08", smaller is better

srmr Standardized Root Mean Square Residual, "Should be < .08", smaller is better



```{r}
if (fitmodels) {
  save(models,file=here(str_replace(str_c("results/",datafile),"\\.data\\.",".models.")))
}
```

